{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileNetEdgeTPU_TFLite.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7MucsWy5WyC"
      },
      "source": [
        "# About\n",
        "TFLite does not support dynamic batch size, so we need to use static batch size.\n",
        "\n",
        "This notebook convert the MobileNetEdgeTPU from TF to TFLite with a static batch size for batch inference.\n",
        "\n",
        "Download and extract this archive (wget does not work with this link):\n",
        "\n",
        "https://storage.cloud.google.com/mobilenet_edgetpu/checkpoints/mobilenet_edgetpu_224_1.0.tgz\n",
        "\n",
        "Then upload the file **frozen_graph_tf1x.pb** into this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK4fpwPC30LU"
      },
      "source": [
        "# Convert TF to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8S5hHDlnlFZ",
        "outputId": "2af34790-e3b0-4d46-8a7a-78ce07d2e4e9"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZez1PyJwDep"
      },
      "source": [
        "batch_size = 3\n",
        "frozen_graph_file = 'frozen_graph_tf1x.pb'\n",
        "tflite_file = f'mobilenet_edgetpu_{batch_size}x224x224x3_1.0_float.tflite'\n",
        "\n",
        "converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\n",
        "    graph_def_file=frozen_graph_file, \n",
        "    input_arrays=['input'],\n",
        "    output_arrays=['MobilenetEdgeTPU/Predictions/Softmax'],\n",
        "    input_shapes={'input': [batch_size, 224, 224, 3]}\n",
        ")\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(tflite_file, 'wb') as f:\n",
        "  f.write(tflite_model)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_479vg434lk"
      },
      "source": [
        "# Validate the converted TFLite model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdq3MMMOms6H",
        "outputId": "c8e874a6-94f9-4850-912c-aa1fffba44f8"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp\n",
        "!curl https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_1.0_224_frozen.tgz  | tar xzv -C ./  mobilenet_v1_1.0_224/labels.txt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-20 10:01:59--  https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/lite/examples/label_image/testdata/grace_hopper.bmp\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 940650 (919K) [image/bmp]\n",
            "Saving to: ‘grace_hopper.bmp.2’\n",
            "\n",
            "\rgrace_hopper.bmp.2    0%[                    ]       0  --.-KB/s               \rgrace_hopper.bmp.2  100%[===================>] 918.60K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-08-20 10:01:59 (26.7 MB/s) - ‘grace_hopper.bmp.2’ saved [940650/940650]\n",
            "\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0mobilenet_v1_1.0_224/labels.txt\n",
            "100 18.2M  100 18.2M    0     0  37.5M      0 --:--:-- --:--:-- --:--:-- 37.4M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6zfFcExm8FL",
        "outputId": "a3260660-f7e1-4eef-eeed-28168837e73e"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "num_threads = 1\n",
        "image_file = 'grace_hopper.bmp'\n",
        "label_file = 'mobilenet_v1_1.0_224/labels.txt'\n",
        "input_mean, input_std = 127.5, 127.5\n",
        "\n",
        "\n",
        "interpreter = tf.lite.Interpreter(\n",
        "      model_path=tflite_file, num_threads=num_threads)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "print(input_details)\n",
        "print(output_details)\n",
        "\n",
        "# check the type of the input tensor\n",
        "floating_model = input_details[0]['dtype'] == np.float32\n",
        "\n",
        "# NxHxWxC, H:1, W:2\n",
        "height = input_details[0]['shape'][1]\n",
        "width = input_details[0]['shape'][2]\n",
        "img = Image.open(image_file).resize((width, height))\n",
        "img_arr = np.array(img)\n",
        "imgs = np.array([img_arr for i in range(batch_size)])\n",
        "print('imgs.shape', imgs.shape)\n",
        "\n",
        "# add N dim\n",
        "input_data = imgs\n",
        "\n",
        "if floating_model:\n",
        "  input_data = (np.float32(input_data) - input_mean) / input_std\n",
        "\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "start_time = time.time()\n",
        "interpreter.invoke()\n",
        "stop_time = time.time()\n",
        "\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(\"output_data.shape\", output_data.shape)\n",
        "\n",
        "with open(label_file, 'r') as f:\n",
        "  labels = [line.strip() for line in f.readlines()]\n",
        "\n",
        "results = np.squeeze(output_data[0])\n",
        "\n",
        "top_k = results.argsort()[-3:][::-1]\n",
        "\n",
        "for i in top_k:\n",
        "  if floating_model:\n",
        "    print('{:08.6f}: {}'.format(float(results[i]), labels[i]))\n",
        "  else:\n",
        "    print('{:08.6f}: {}'.format(float(results[i] / 255.0), labels[i]))\n",
        "\n",
        "print('time: {:.3f}ms'.format((stop_time - start_time) * 1000))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'name': 'input', 'index': 0, 'shape': array([  3, 224, 224,   3], dtype=int32), 'shape_signature': array([  3, 224, 224,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "[{'name': 'MobilenetEdgeTPU/Predictions/Softmax', 'index': 196, 'shape': array([   3, 1001], dtype=int32), 'shape_signature': array([   3, 1001], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "imgs.shape (3, 224, 224, 3)\n",
            "output_data.shape (3, 1001)\n",
            "0.814296: 653:military uniform\n",
            "0.059763: 458:bow tie, bow-tie, bowtie\n",
            "0.014469: 835:suit, suit of clothes\n",
            "time: 150.815ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCJP4yXYqI2Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}