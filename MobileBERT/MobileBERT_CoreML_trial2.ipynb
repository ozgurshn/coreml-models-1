{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileBERT_CoreML_trial2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEpLAS_sBuqD"
      },
      "source": [
        "# About\n",
        "Convert MobileBERT_SQuAD to Core ML\n",
        "\n",
        "Convert SavedModel to ConcreteFunction then convert ConreteFunction to Core ML.\n",
        "\n",
        "**Quantization**:\n",
        "16 bits worked, but 8 bits quantized model failed to load later in iOS with error:\n",
        "\n",
        "```\n",
        "/Library/Caches/com.apple.xbs/Sources/MetalImage/MetalImage-124.2.4/MPSCore/Types/MPSMatrix.mm, line 222: error '[MPSMatrix initWithBuffer:descriptor:] not enough rowBytes for all the columns.'\n",
        "/Library/Caches/com.apple.xbs/Sources/MetalImage/MetalImage-124.2.4/MPSCore/Types/MPSMatrix.mm:222: failed assertion `[MPSMatrix initWithBuffer:descriptor:] not enough rowBytes for all the columns.'\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ8Uv_p5aq13"
      },
      "source": [
        "# Download model and setup enviroments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Wf8eJU5cI9v",
        "outputId": "7bad4c30-3fcd-4eba-b75c-a99148a6b02e"
      },
      "source": [
        "!wget https://storage.googleapis.com/cloud-tpu-checkpoints/mobilebert/mobilebert_squad_savedmodels.tar.gz\n",
        "!tar -zxvf mobilebert_squad_savedmodels.tar.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-18 12:38:45--  https://storage.googleapis.com/cloud-tpu-checkpoints/mobilebert/mobilebert_squad_savedmodels.tar.gz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.188.128, 64.233.189.128, 108.177.97.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.188.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 184730530 (176M) [application/octet-stream]\n",
            "Saving to: ‘mobilebert_squad_savedmodels.tar.gz.1’\n",
            "\n",
            "mobilebert_squad_sa 100%[===================>] 176.17M  31.5MB/s    in 5.6s    \n",
            "\n",
            "2021-08-18 12:38:53 (31.5 MB/s) - ‘mobilebert_squad_savedmodels.tar.gz.1’ saved [184730530/184730530]\n",
            "\n",
            "mobilebert_squad_savedmodels/\n",
            "mobilebert_squad_savedmodels/quant_saved_model/\n",
            "mobilebert_squad_savedmodels/quant_saved_model/saved_model.pb\n",
            "mobilebert_squad_savedmodels/quant_saved_model/variables/\n",
            "mobilebert_squad_savedmodels/quant_saved_model/variables/variables.index\n",
            "mobilebert_squad_savedmodels/quant_saved_model/variables/variables.data-00000-of-00001\n",
            "mobilebert_squad_savedmodels/float/\n",
            "mobilebert_squad_savedmodels/float/saved_model.pb\n",
            "mobilebert_squad_savedmodels/float/variables/\n",
            "mobilebert_squad_savedmodels/float/variables/variables.index\n",
            "mobilebert_squad_savedmodels/float/variables/variables.data-00000-of-00001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qJtFzxhZ1Sc",
        "outputId": "23e0c7b5-05a3-4bcc-a5ca-e2ae82ca7142"
      },
      "source": [
        "!pip install coremltools==4.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: coremltools==4.1 in /usr/local/lib/python3.7/dist-packages (4.1)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from coremltools==4.1) (21.2.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from coremltools==4.1) (1.7.1)\n",
            "Requirement already satisfied: numpy<1.20,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from coremltools==4.1) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from coremltools==4.1) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from coremltools==4.1) (3.17.3)\n",
            "Requirement already satisfied: attr in /usr/local/lib/python3.7/dist-packages (from coremltools==4.1) (0.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from coremltools==4.1) (21.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from coremltools==4.1) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from coremltools==4.1) (4.62.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->coremltools==4.1) (2.4.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->coremltools==4.1) (1.2.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj9YT1x6akk-"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7efqejtalVc",
        "outputId": "ffd84d8e-354a-4b13-de74-769d32538450"
      },
      "source": [
        "import sys\n",
        "print(sys.version)\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import coremltools as ct\n",
        "print(ct.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.7.11 (default, Jul  3 2021, 18:01:19) \n",
            "[GCC 7.5.0]\n",
            "2.6.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:scikit-learn version 0.22.2.post1 is not supported. Minimum required version: 0.17. Maximum required version: 0.19.2. Disabling scikit-learn conversion API.\n",
            "WARNING:root:TensorFlow version 2.6.0 detected. Last version known to be fully compatible is 2.3.1 .\n",
            "WARNING:root:Keras version 2.6.0 detected. Last version known to be fully compatible of Keras is 2.2.4 .\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1dwjjj35tj7"
      },
      "source": [
        "# Inspect the TF model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUsXzy2V5JYm",
        "outputId": "d5e25a0c-9dcd-4d0d-e6fb-bd31f9be53c3"
      },
      "source": [
        "def load_saved_model(path):\n",
        "  print(f\"Loading saved_model.pb from {path}\")\n",
        "  the_graph = tf.Graph()\n",
        "  with tf.compat.v1.Session(graph=the_graph) as sess:\n",
        "    tags = [tf.compat.v1.saved_model.tag_constants.SERVING]\n",
        "    tf.compat.v1.saved_model.loader.load(sess, tags, path)\n",
        "  return the_graph\n",
        "\n",
        "\n",
        "def export_ops_name(the_graph, filename):\n",
        "  with open(filename, \"w\") as text_file:\n",
        "    ops = the_graph.get_operations()\n",
        "    N = len(ops)\n",
        "    for i in range(N):\n",
        "      text_file.write('\\n\\nop id {} , op type: \"{}\"'.format(str(i), ops[i].type))\n",
        "      \n",
        "      text_file.write('\\ninput(s):'),\n",
        "      for x in ops[i].inputs:\n",
        "        text_file.write(\"name = {}, shape: {}, \".format(x.name, x.get_shape()))\n",
        "      \n",
        "      text_file.write('\\noutput(s):'),\n",
        "      for x in ops[i].outputs:\n",
        "        text_file.write(\"name = {}, shape: {},\".format(x.name, x.get_shape()))\n",
        "  print('Exported to:', filename)    \n",
        "\n",
        "\n",
        "saved_model_dir = 'mobilebert_squad_savedmodels/float'\n",
        "\n",
        "export_ops_name(load_saved_model(saved_model_dir),\n",
        "                'mobilebert_squad_savedmodels_ops.txt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading saved_model.pb from mobilebert_squad_savedmodels/float\n",
            "Exported to: mobilebert_squad_savedmodels_ops.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbh5po2_axMd"
      },
      "source": [
        "# Convert from TF to Core ML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxlauX1bEKr_",
        "outputId": "3202d01c-8997-4cd4-c3ba-a12df15616d6"
      },
      "source": [
        "tfmodel = tf.saved_model.load(saved_model_dir, tags='serve')\n",
        "print(tfmodel)\n",
        "\n",
        "sv = tfmodel.signatures.values()\n",
        "print(sv)\n",
        "\n",
        "cfs = sv if isinstance(sv, list) else list(sv)\n",
        "print(cfs)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.training.tracking.tracking.AutoTrackable object at 0x7fae78e560d0>\n",
            "ValuesView(_SignatureMap({'serving_default': <ConcreteFunction pruned(input_ids, input_mask, segment_ids) at 0x7FADBB1BD7D0>}))\n",
            "[<ConcreteFunction pruned(input_ids, input_mask, segment_ids) at 0x7FADBB1BD7D0>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csExJ507anVE",
        "outputId": "b54abc76-dfa4-4bf6-c9fc-5101bcb13dde"
      },
      "source": [
        "mlmodel = ct.convert(cfs, source='TensorFlow')\n",
        "\n",
        "print(mlmodel)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running TensorFlow Graph Passes: 100%|██████████| 5/5 [00:03<00:00,  1.49 passes/s]\n",
            "Converting Frontend ==> MIL Ops: 100%|██████████| 4430/4430 [00:18<00:00, 242.95 ops/s]\n",
            "Running MIL optimization passes: 100%|██████████| 18/18 [00:15<00:00,  1.14 passes/s]\n",
            "Translating MIL ==> MLModel Ops: 100%|██████████| 4271/4271 [00:04<00:00, 988.88 ops/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "input {\n",
            "  name: \"input_ids\"\n",
            "  type {\n",
            "    multiArrayType {\n",
            "      shape: 1\n",
            "      shape: 384\n",
            "      dataType: FLOAT32\n",
            "    }\n",
            "  }\n",
            "}\n",
            "input {\n",
            "  name: \"input_mask\"\n",
            "  type {\n",
            "    multiArrayType {\n",
            "      shape: 1\n",
            "      shape: 384\n",
            "      dataType: FLOAT32\n",
            "    }\n",
            "  }\n",
            "}\n",
            "input {\n",
            "  name: \"segment_ids\"\n",
            "  type {\n",
            "    multiArrayType {\n",
            "      shape: 1\n",
            "      shape: 384\n",
            "      dataType: FLOAT32\n",
            "    }\n",
            "  }\n",
            "}\n",
            "output {\n",
            "  name: \"end_logits\"\n",
            "  type {\n",
            "    multiArrayType {\n",
            "      dataType: FLOAT32\n",
            "    }\n",
            "  }\n",
            "}\n",
            "output {\n",
            "  name: \"start_logits\"\n",
            "  type {\n",
            "    multiArrayType {\n",
            "      dataType: FLOAT32\n",
            "    }\n",
            "  }\n",
            "}\n",
            "metadata {\n",
            "  userDefined {\n",
            "    key: \"com.github.apple.coremltools.source\"\n",
            "    value: \"tensorflow==2.6.0\"\n",
            "  }\n",
            "  userDefined {\n",
            "    key: \"com.github.apple.coremltools.version\"\n",
            "    value: \"4.1\"\n",
            "  }\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ackxj4aqy9-",
        "outputId": "0b320ab3-0eb8-4b2c-f288-0e509ccce20f"
      },
      "source": [
        "from coremltools.models.neural_network import quantization_utils\n",
        "\n",
        "spec = quantization_utils.quantize_weights(mlmodel, nbits=16)\n",
        "\n",
        "mlmodel = ct.models.MLModel(spec)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Quantizing using linear quantization\n",
            "Quantizing layer bert/embeddings/embedding_lookup_1\n",
            "Quantizing layer bert/embeddings/embedding_lookup\n",
            "Quantizing layer bert/embeddings/embedding_transformation/add\n",
            "Quantizing layer bert/encoder/layer_0/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_0/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_0/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_0/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_0/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_0/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_0/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_0/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_0/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_0/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_0/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_0/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_0/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_0/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_0/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_1/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_1/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_1/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_1/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_1/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_1/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_1/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_1/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_1/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_1/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_1/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_1/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_1/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_1/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_1/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_2/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_2/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_2/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_2/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_2/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_2/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_2/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_2/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_2/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_2/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_2/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_2/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_2/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_2/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_2/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_3/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_3/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_3/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_3/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_3/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_3/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_3/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_3/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_3/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_3/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_3/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_3/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_3/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_3/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_3/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_3/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_3/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_4/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_4/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_4/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_4/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_4/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_4/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_4/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_4/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_4/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_4/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_4/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_4/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_4/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_4/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_4/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_4/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_4/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_5/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_5/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_5/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_5/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_5/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_5/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_5/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_5/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_5/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_5/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_5/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_5/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_5/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_5/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_5/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_5/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_5/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_6/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_6/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_6/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_6/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_6/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_6/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_6/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_6/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_6/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_6/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_6/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_6/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_6/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_6/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_6/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_6/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_6/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_7/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_7/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_7/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_7/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_7/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_7/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_7/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_7/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_7/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_7/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_7/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_7/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_7/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_7/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_7/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_7/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_7/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_8/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_8/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_8/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_8/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_8/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_8/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_8/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_8/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_8/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_8/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_8/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_8/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_8/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_8/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_8/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_8/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_8/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_9/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_9/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_9/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_9/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_9/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_9/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_9/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_9/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_9/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_9/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_9/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_9/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_9/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_9/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_9/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_9/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_9/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_10/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_10/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_10/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_10/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_10/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_10/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_10/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_10/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_10/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_10/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_10/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_10/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_10/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_10/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_10/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_10/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_10/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_11/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_11/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_11/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_11/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_11/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_11/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_11/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_11/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_11/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_11/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_11/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_11/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_11/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_11/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_11/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_11/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_11/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_12/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_12/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_12/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_12/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_12/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_12/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_12/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_12/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_12/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_12/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_12/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_12/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_12/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_12/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_12/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_12/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_12/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_13/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_13/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_13/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_13/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_13/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_13/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_13/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_13/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_13/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_13/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_13/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_13/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_13/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_13/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_13/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_13/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_13/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_14/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_14/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_14/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_14/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_14/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_14/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_14/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_14/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_14/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_14/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_14/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_14/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_14/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_14/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_14/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_14/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_14/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_15/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_15/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_15/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_15/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_15/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_15/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_15/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_15/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_15/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_15/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_15/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_15/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_15/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_15/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_15/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_15/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_15/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_16/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_16/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_16/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_16/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_16/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_16/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_16/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_16/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_16/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_16/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_16/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_16/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_16/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_16/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_16/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_16/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_16/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_17/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_17/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_17/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_17/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_17/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_17/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_17/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_17/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_17/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_17/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_17/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_17/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_17/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_17/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_17/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_17/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_17/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_18/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_18/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_18/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_18/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_18/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_18/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_18/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_18/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_18/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_18/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_18/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_18/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_18/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_18/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_18/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_18/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_18/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_19/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_19/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_19/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_19/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_19/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_19/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_19/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_19/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_19/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_19/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_19/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_19/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_19/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_19/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_19/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_19/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_19/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_20/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_20/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_20/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_20/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_20/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_20/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_20/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_20/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_20/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_20/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_20/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_20/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_20/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_20/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_20/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_20/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_20/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_21/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_21/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_21/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_21/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_21/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_21/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_21/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_21/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_21/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_21/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_21/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_21/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_21/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_21/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_21/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_21/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_21/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_22/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_22/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_22/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_22/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_22/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_22/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_22/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_22/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_22/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_22/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_22/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_22/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_22/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_22/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_22/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_22/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_22/output/bottleneck/dense/add\n",
            "Quantizing layer bert/encoder/layer_23/bottleneck/attention/dense/Tensordot/MatMul\n",
            "Quantizing layer bert/encoder/layer_23/attention/self/value/add\n",
            "Quantizing layer bert/encoder/layer_23/bottleneck/input/dense/add\n",
            "Quantizing layer bert/encoder/layer_23/attention/self/query/add\n",
            "Quantizing layer bert/encoder/layer_23/attention/self/key/add\n",
            "Quantizing layer bert/encoder/layer_23/attention/self/MatMul\n",
            "Quantizing layer bert/encoder/layer_23/attention/self/MatMul_1\n",
            "Quantizing layer bert/encoder/layer_23/attention/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_23/ffn_layer_0/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_23/ffn_layer_0/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_23/ffn_layer_1/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_23/ffn_layer_1/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_23/ffn_layer_2/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_23/ffn_layer_2/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_23/intermediate/dense/add\n",
            "Quantizing layer bert/encoder/layer_23/output/dense/add\n",
            "Quantizing layer bert/encoder/layer_23/output/bottleneck/dense/add\n",
            "Quantizing layer BiasAdd\n",
            "WARNING! Unable to return a quantized MLModel instance sinceOS != macOS 10.14 or later\n",
            "Returning quantized model specification instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kML5IYiDavve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad491a7-9c47-4f16-984b-50d731b9b71b"
      },
      "source": [
        "# rename input, ouput\n",
        "mlmodel.input_description['input_ids'] = 'an int32 Tensor of shape [seq_length] with the token ids of the packed input sequence (that is, including a start-of-sequence token, end-of-segment tokens, and padding).'\n",
        "mlmodel.input_description['input_mask'] = 'an int32 Tensor of shape [seq_length] with value 1 at the position of all input tokens present before padding and value 0 for the padding tokens.'\n",
        "mlmodel.input_description['segment_ids'] = 'an int32 Tensor of shape [seq_length] with the index of the input segment that gave rise to the input token at the respective position. The first input segment (index 0) includes the start-of-sequence token and its end-of-segment token. The second and later segments (if present) include their respetive end-of-segment token. Padding tokens get index 0 again.'\n",
        "\n",
        "mlmodel.output_description['start_logits'] = \"Start token scores of size 384. The argmax is the start index of the predicted answer in the input sequence\"\n",
        "mlmodel.output_description['end_logits'] = \"End token scores of size 384. The argmax is the end index of the predicted answer in the input sequence\"\n",
        "\n",
        "spec = mlmodel.get_spec()\n",
        "\n",
        "ct.utils.rename_feature(spec, 'input_ids', 'inputIds')\n",
        "ct.utils.rename_feature(spec, 'input_mask', 'inputMask')\n",
        "ct.utils.rename_feature(spec, 'segment_ids', 'segmentIds')\n",
        "\n",
        "ct.utils.rename_feature(spec, 'start_logits', 'startLogits')\n",
        "ct.utils.rename_feature(spec, 'end_logits', 'endLogits')\n",
        "\n",
        "# set shape info\n",
        "\n",
        "del spec.description.input[0].type.multiArrayType.shape[0]\n",
        "del spec.description.input[1].type.multiArrayType.shape[0]\n",
        "del spec.description.input[2].type.multiArrayType.shape[0]\n",
        "\n",
        "spec.description.input[0].type.multiArrayType.dataType = ct.proto.FeatureTypes_pb2.ArrayFeatureType.INT32\n",
        "spec.description.input[1].type.multiArrayType.dataType = ct.proto.FeatureTypes_pb2.ArrayFeatureType.INT32\n",
        "spec.description.input[2].type.multiArrayType.dataType = ct.proto.FeatureTypes_pb2.ArrayFeatureType.INT32\n",
        "\n",
        "seq_length = 384\n",
        "spec.description.output[0].type.multiArrayType.shape.append(seq_length)\n",
        "spec.description.output[1].type.multiArrayType.shape.append(seq_length)\n",
        "\n",
        "spec.description.output[0].type.multiArrayType.dataType = ct.proto.FeatureTypes_pb2.ArrayFeatureType.DOUBLE\n",
        "spec.description.output[1].type.multiArrayType.dataType = ct.proto.FeatureTypes_pb2.ArrayFeatureType.DOUBLE\n",
        "\n",
        "# set model info\n",
        "spec.description.metadata.versionString = \"2021-08-18\"\n",
        "spec.description.metadata.shortDescription = \"Converted from TF model at https://storage.googleapis.com/cloud-tpu-checkpoints/mobilebert/mobilebert_squad_savedmodels.tar.gz\"\n",
        "spec.description.metadata.author = \"Converted to Core ML by Anh\"\n",
        "\n",
        "mlmodel_mod = ct.models.MLModel(spec)\n",
        "mlmodel_mod.save('MobileBERT_SQuAD.mlmodel')\n",
        "print(mlmodel_mod)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input {\n",
            "  name: \"inputIds\"\n",
            "  shortDescription: \"an int32 Tensor of shape [seq_length] with the token ids of the packed input sequence (that is, including a start-of-sequence token, end-of-segment tokens, and padding).\"\n",
            "  type {\n",
            "    multiArrayType {\n",
            "      shape: 384\n",
            "      dataType: INT32\n",
            "    }\n",
            "  }\n",
            "}\n",
            "input {\n",
            "  name: \"inputMask\"\n",
            "  shortDescription: \"an int32 Tensor of shape [seq_length] with value 1 at the position of all input tokens present before padding and value 0 for the padding tokens.\"\n",
            "  type {\n",
            "    multiArrayType {\n",
            "      shape: 384\n",
            "      dataType: INT32\n",
            "    }\n",
            "  }\n",
            "}\n",
            "input {\n",
            "  name: \"segmentIds\"\n",
            "  shortDescription: \"an int32 Tensor of shape [seq_length] with the index of the input segment that gave rise to the input token at the respective position. The first input segment (index 0) includes the start-of-sequence token and its end-of-segment token. The second and later segments (if present) include their respetive end-of-segment token. Padding tokens get index 0 again.\"\n",
            "  type {\n",
            "    multiArrayType {\n",
            "      shape: 384\n",
            "      dataType: INT32\n",
            "    }\n",
            "  }\n",
            "}\n",
            "output {\n",
            "  name: \"endLogits\"\n",
            "  shortDescription: \"End token scores of size 384. The argmax is the end index of the predicted answer in the input sequence\"\n",
            "  type {\n",
            "    multiArrayType {\n",
            "      shape: 384\n",
            "      dataType: DOUBLE\n",
            "    }\n",
            "  }\n",
            "}\n",
            "output {\n",
            "  name: \"startLogits\"\n",
            "  shortDescription: \"Start token scores of size 384. The argmax is the start index of the predicted answer in the input sequence\"\n",
            "  type {\n",
            "    multiArrayType {\n",
            "      shape: 384\n",
            "      dataType: DOUBLE\n",
            "    }\n",
            "  }\n",
            "}\n",
            "metadata {\n",
            "  shortDescription: \"Converted from TF model at https://storage.googleapis.com/cloud-tpu-checkpoints/mobilebert/mobilebert_squad_savedmodels.tar.gz\"\n",
            "  versionString: \"2021-08-18\"\n",
            "  author: \"Converted to Core ML by Anh\"\n",
            "  userDefined {\n",
            "    key: \"com.github.apple.coremltools.source\"\n",
            "    value: \"tensorflow==2.6.0\"\n",
            "  }\n",
            "  userDefined {\n",
            "    key: \"com.github.apple.coremltools.version\"\n",
            "    value: \"4.1\"\n",
            "  }\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXpuUCVUs-Fb"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}